---
title: "Rendu Travaux Pratiques 5"
output:
  html_document: default
  html_notebook: default
---

** **

#### Règles de rendu

* Chaque TP donne lieu à un bref compte-rendu portant sur certaines questions posées dans l'énoncé du TPs.

* Le compte-rendu doit être complété à partir du texte de l'énoncé. Les codes R doivent être inclus dans le texte du compte-rendu (menu **Insert**) et commentés avec précision. **Les commentaires compteront pour une part importante dans la note**.

* Le compte-rendu doit être déposé **sur TEIDE à la fin de la séance de TP**. Les rendus en retard seront fortement pénalisés. 

* Le compte-rendu doit être déposé **sur TEIDE au format HTML uniquement**. Utiliser la fonction **Preview** ou **knitr** du menu de rstudio pour obtenir le document au format souhaité. **Les fichiers "source" (Rmd) ne seront pas acceptés par les correcteurs**.

Le dépot comportera en plus du fichier intitulé "Rendu_TP5.html" 

- un "objet" R sauvé sous forme compressée contenant le modèle choisi pour être évalué par l'enseignant sur l'ensemble test. Le fichier compressé devra être sauvé sous le format RDS ou sous le format hdf5. son intitulé devra être personnalisé sous la forme "my_login_bestmodel.RDS" ou "my_login_bestmodel.hdf5".

- Un script R, contenant une fonction permettant d'évaluer le modèle.
 
```{r}
library(magrittr)
library(keras)
# install_keras()
# install_keras(tensorflow="gpu")
```

** **
## Exercice : Défi analyse d'opinion (IMDB)

#### Analyse d'association

```{r}
# IMDB Word Index
index <- keras::dataset_imdb_word_index()

# Sorted
o <- as.numeric(index) %>% order()

# Documents
imbd <- keras::dataset_imdb(path = "imdb.npz", 
                            num_words = 2048, 
                            skip_top = 48)

x_imbd <- NULL
for (i in 1:10){
  x_imbd_500 <- NULL
  for (j in (500*(i-1)+1):(500*i)){
    
    # x_imdb_500 reçoit pour chaque document de chaque paquets de 500 documents du total des 5000 documents, les occurences des 2000 mots les plus fréquents. (Autrement dit 10000000 mots par itération). Ensuite on ajoute le total dans x_imbd (10000000 mots en 5000 documents)
    
    doc_temp <- imbd$train$x[[j]]
    x_imbd_500 <- rbind(x_imbd_500, sapply(49:2048, FUN = function(ind) sum(doc_temp == ind)))
    # if (j%%500 == 0) print(j)
  }
  x_imbd <- rbind(x_imbd, x_imbd_500)
}

for (i in 1:10){
  x_imbd_500 <- NULL
  for (j in (500*(i-1)+1):(500*i)){
    
    # x_imdb_500 reçoit pour chaque document de chaque paquets de 500 documents du total des 5000 documents, les occurences des 2000 mots les plus fréquents. (Autrement dit 10000000 mots par itération). Ensuite on ajoute le total dans x_imbd (20000000 mots en 10000 documents) 
    
    doc_temp <- imbd$test$x[[j]]
    x_imbd_500 <- rbind(x_imbd_500, sapply(49:2048,FUN = function(ind) sum(doc_temp == ind)))
    # if (j%%500 == 0) print(j) # ca rassure
  }
  x_imbd <- rbind(x_imbd, x_imbd_500)
}


y_imbd <- to_categorical(imbd$train$y[1:5000], 2)
y_imbd <- rbind(y_imbd, to_categorical(imbd$test$y[1:5000], 2))
```

* Dans quelles proportions les termes de valeur d'association $r^2$ supérieure à 0.02 apparaissent-ils dans les documents ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre. 
```{r}
  # Fréquences d'apparition des termes de l'index
  x <- x_imbd[,-2000]

  # Opinion des utilisateurs
  y <- y_imbd[,1]
  
  # Calcul du coefficient de correlation au carre des deux ensembles
  r2 <- cor(x, y)^2
  
# Calcul de la frequence des termes realisant la condition
  freq <- x[, which(r2 > 0.02) + 45] %>% apply(2, mean) 

# Diagramme en barre réalisant la conditions
  names(freq) <-  index[o[which(r2 > 0.02) + 45]] %>% names() 
  barplot(freq, col = "lightblue", las = 2)
```

* Dans quelles proportions les termes de valeur d'association $r^2$ supérieure à 0.02 apparaissent-ils dans les documents **à connotation positive** ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre.

```{r}
# Calculer la frequence des termes realisant la condition
  freq <- x[y == 1, r2 > 0.02] %>% apply(2, mean)
# mots dans l'index et barplot
  names(freq) <-  index[o[(which(r2 > 0.02) + 45)]] %>% names()
  barplot(freq, col = "lightblue", las = 2)
```

#### Modèles de prédiction

* \`A l'aide des outils vus dans les séances précédentes, tels que `keras`, `lda`, `nnet`, ou d'autres bibliothèques de programmes que vous pourriez trouver dans R, ajuster des modèles d'apprentissage aux données contenues dans le TP : `x_imbd` et `y_imbd`.

```{r}
# On retranche les données en ensembles de tests et d'apprentissage
  x_train <- x_imbd[0:5000,]
  x_test <- x_imbd[5001:10000,]
  y_train <- y_imbd[0:5000,]
  y_test <- y_imbd[5001:10000,]
```

Méthode analyse discriminante linéaire:
```{r}
    # lda de l'ensemble d'apprentissage
      # mod_lda <- MASS::lda(y_train ~ . , data = x_train)
    mod_lda <- MASS::lda(x = x_train[,-2000], grouping = y_train[,-1])

    # probabilité à priori de l'ensemble d'apprentissage
      prediction <- predict(mod_lda, newdata = x_test[, -2000])
      pred_lda <- prediction$class
      
    # précision entre les 2 ensemble (apprentissage et test)
      accuracy_lda <- mean(pred_lda == y_test[,-1])
      
    # probabilité de l'ensemble testé
      prob_lda <- prediction$posterior
      
      prob_lda[prob_lda == 1] <- 1 - 1e-09
      prob_lda[prob_lda == 0] <- 1e-09 
      
    # logloss 
      log_loss_lda <- -mean((y_test[,-1] == 1)*log(1 - prob_lda[1]) + (y_test[,-1] == 0)*log(prob_lda[1]))
      
      accuracy_lda
      log_loss_lda
```

Méthode du réseau de neuronnes (multiples couches cachées):

```{r}
library(keras)
model <- keras_model_sequential() 
model %>% 
    layer_dense(units = 5, activation = 'relu', input_shape = 2000) %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(5, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(5, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(5, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 2, activation = 'sigmoid')
  
## On redéfinie le modèle pour obtenir les valeurs voulues (logloss, accuracy)
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

## On applique l'entrainement sur 10 générations
history <- model %>% fit(
                    x_train, 
                    y_train, 
                    epochs = 10,
                    batch_size = 128,
                    validation_split = 0.1
)
```
```{r}
eval_keras <- model %>% evaluate(x_test, y_test)
loss_keras <- eval_keras$loss
acc_keras <- eval_keras$acc
```

Méthode du réseau de neuronnes (1 couche cachée):

```{r}
library(keras)
model_keras <- keras_model_sequential() 
model_keras %>% 
    layer_dense(units = 5, activation = 'relu', input_shape = 2000) %>% 
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 2, activation = 'softmax')
  
## On redéfinie le modèle pour obtenir les valeurs voulues (logloss, accuracy)
model_keras %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(lr = 0.001, decay = 0.00001),
  metrics = c('accuracy')
)

## On applique l'entrainement sur 10 générations
history_non_deep <- model_keras %>% fit(
                    x = x_train, 
                    y = y_train, 
                    epochs = 10,
                    batch_size = 128,
                    validation_split = 0.1
)
```
```{r}
eval_keras_non_deep <- model_keras %>% evaluate(x_test, y_test)
loss_keras_non_deep <- eval_keras_non_deep$loss
acc_keras_non_deep <- eval_keras_non_deep$acc
```

Méthode du réseau de neuronnes (0 couche cachée):

```{r, include=FALSE}
library(keras)
model_keras_zero_hidden <- keras_model_sequential() 
model_keras_zero_hidden %>% 
    layer_dense(units = 2, activation = 'sigmoid', input_shape = 2000)
  
## On redéfinie le modèle pour obtenir les valeurs voulues (logloss, accuracy)
model_keras_zero_hidden %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(lr = 0.001, decay = 0.00001),
  metrics = c('accuracy')
)

## On applique l'entrainement sur 10 générations
history_zero_hidden <- model_keras_zero_hidden %>% fit(
                    x = x_train, 
                    y = y_train, 
                    epochs = 10,
                    batch_size = 128,
                    validation_split = 0.1
)
```
```{r}
eval_keras_zero_hidden <- model_keras_zero_hidden %>% evaluate(x_test, y_test)
loss_keras_zero_hidden <- eval_keras_zero_hidden$loss
acc_keras_zero_hidden <- eval_keras_zero_hidden$acc
```

Méthode du réseau de neuronnes (nnet):

```{r}
    # nnet
  require(nnet)
  mod_nnet <- nnet(x = x_train, 
                   y = y_train[,1], 
                   size = 2,
                   entropy = TRUE, 
                   decay = 0.01,
                   MaxNWts = 10020,
                   trace = FALSE)
  
  prob_nnet <- predict(mod_nnet, x_test)
  
  prob_nnet[prob_nnet == 0] <- 1e-08
  prob_nnet[prob_nnet == 1] <- 1 - 1e-08
  
  accuracy_nnet <- mean((prob_nnet > 0.5) == (y_test[,1] == 1))

  log_loss_nnet <- -mean((y_test[,1] == 0)*log(1 - prob_nnet) + (y_test[,1] == 1)*log(prob_nnet))
```

Méthode des k-plus-proches-voisins

```{r}
    # knn
  mod_knn <- class::knn(train = x_train, 
                          test = x_test, 
                          cl = y_train[,1], 
                          k = 5, 
                          prob = TRUE)
```
```{r}
  accuracy_knn <- mean(mod_knn == y_test[,1])
  
  prob_knn <- attr(mod_knn, "prob")
  prob_knn[prob_knn == 1] <-  1 - 1e-08
    
  log_loss_knn <- - mean( (mod_knn == y_test[,1])*log(prob_knn) + (mod_knn != y_test[,1])*log(1 - prob_knn) )
``` 

* Dans un tableau, décrire les performances des méthodes choisies pour des échantillons d'apprentissage et de test que vous aurez créés vous-mêmes à partir des objets `x_imbd` et `y_imbd`. Les performances seront mesurées par les erreurs de classification et d'entropie (perte log loss).

```{r}
  log_loss <-  c(loss_keras, 
                 loss_keras_non_deep,
                 loss_keras_zero_hidden,
                 log_loss_lda, 
                 log_loss_nnet, 
                 log_loss_knn)


  names_models <- c("deep_nnet", "non_deep_nnet", "0_hidden_layer_nnet", "lda", "nnet", 'knn')

  names(log_loss) <-  names_models
  
  accuracy <- c(acc_keras, 
                acc_keras_non_deep, 
                acc_keras_zero_hidden,
                accuracy_lda, 
                accuracy_nnet, 
                accuracy_knn)
  
  names(accuracy) <-  names_models
  
  data.frame(accuracy, log_loss)  %>% knitr::kable(digit = 2)
```

* Donner le code R correspondant au meilleur modèle que vous avez ajusté (chunck ci-dessous). 

```{r}
# code de mon meilleur modèle

model_keras <- keras_model_sequential() 
model_keras %>% 
    layer_dense(units = 5, activation = 'relu', input_shape = 2000) %>% 
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 2, activation = 'softmax')
  
## On redéfinie le modèle pour obtenir les valeurs voulues (logloss, accuracy)
model_keras %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(lr = 0.001, decay = 0.00001),
  metrics = c('accuracy')
)

## On applique l'entrainement sur 10 générations
history_non_deep <- model_keras %>% fit(
                    x = x_train, 
                    y = y_train, 
                    epochs = 10,
                    batch_size = 128,
                    validation_split = 0.1
)
model <-  model_keras
```


* Sauver votre meilleur modèle dans un format compressé (RDS ou HDF5 pour keras). Remplacer la chaine de caractère "my_login" par votre propre login ensimag. 

```{r}
#keras
  save_model_hdf5(object = model, filepath = "nguyene_model_keras.hdf5")
```

* Déposer votre modèle dans TEIDE. 

* Ecrire et appliquer une fonction appelée "prediction_finale" pouvant prendre par défaut en entrée une matrice appelée "x_ultime" de taille 5000 lignes et 2000 colonnes contenant des valeurs binaires et une matrice "y_ultime" de taille 5000 lignes et 2 colonnes contenant des valeurs binaires. Cette fonction devra prédire les classes contenues dans "y_ultime" à partir des données "x_ultime" en chargeant le modèle que vous aurez choisi pour le défi. Elle calculera les taux de classification et la perte log loss pour l'ensemble considéré.

```{r}
prediction_finale <- function(x_ultime = "x_ultime", 
                              y_ultime = "y_ultime",
                              file_path = "nguyene_model_keras.hdf5"){
  #Remplacer "my_login" par votre propre login ensimag. 
  
  require(magrittr) 
  
  #tests 
  if (nrow(x_ultime) != 5000 | ncol(x_ultime) != 2000) 
    stop("Dimensions de x incorrectes.")

  if (nrow(y_ultime) != 5000 | ncol(y_ultime) != 2) 
    stop("Dimensions de y incorrectes.")
    
  #if keras
    require(keras) 

    load_model_hdf5(filepath = file_path)
    model_keras %>% evaluate(x_ultime, y_ultime)
}
```

```{r}
 prediction_finale(x_test, y_test)
```

* Joindre un script R contenant la fonction `prediction_finale()` préalablement testée

* Ne pas oublier la version html de ce compte rendu.
